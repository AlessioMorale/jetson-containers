# Copyright (c) 2021, NVIDIA CORPORATION. All rights reserved.
#
# Permission is hereby granted, free of charge, to any person obtaining a
# copy of this software and associated documentation files (the "Software"),
# to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense,
# and/or sell copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
# DEALINGS IN THE SOFTWARE.

ARG BASE_IMAGE
FROM ${BASE_IMAGE}

ENV DEBIAN_FRONTEND=noninteractive


#
# instructions from:  https://github.com/triton-inference-server/server/releases/tag/v2.8.0
#
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
			software-properties-common \
			autoconf \
			automake \
			build-essential \
			cmake \
			git \
			libb64-dev \
			libre2-dev \
			libssl-dev \
			libtool \
			libboost-dev \
			libcurl4-openssl-dev \
			rapidjson-dev \
			patchelf \
			zlib1g-dev \
			curl \
			pkg-config \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

RUN pip3 install grpcio-tools future --verbose


#
# Triton inference server
#
ARG TRITON_URL
ARG TRITON_TGZ
ARG TRITON_WHL

RUN cd /opt && \
    wget --quiet --show-progress --progress=bar:force:noscroll --no-check-certificate ${TRITON_URL} -O ${TRITON_TGZ} && \
    tar -xzvf ${TRITON_TGZ} --one-top-level=tritonserver && \
    ls tritonserver && \
    pip3 install tritonserver/${TRITON_WHL}[all] --verbose && \
    rm ${TRITON_TGZ}

ENV PATH="/opt/tritonserver/bin:${PATH}"
ENV LD_LIBRARY_PATH="/opt/tritonserver/lib:${LD_LIBRARY_PATH}"


#
# PyTorch backend
# https://github.com/triton-inference-server/pytorch_backend#build-the-pytorch-backend-with-custom-pytorch
#
ARG TRITON_PYTORCH_BACKEND_VERSION

RUN apt-get update && \
    apt-get install -y --no-install-recommends \
			rapidjson-dev \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean
    
# pytorch_backend requires cmake >= 3.18
RUN pip3 install scikit-build && \
    pip3 install ninja && \
    pip3 install cmake --upgrade --verbose && \
    cmake --version

# patch for pytorch_backend to disable MKL on aarch64
COPY packages/tritonserver/pytorch_backend/CMakeLists.txt /tmp/tritonserver/pytorch_backend/CMakeLists.txt

# triton needs the torchvision headers, but they aren't in the torchvision wheel
# so checkout the source with same version that is already installed into the container
RUN cd /tmp && \
    TORCHVISION_VERSION=`pip3 show torchvision | grep Version: | cut -d' ' -f2` && \
    TORCHVISION_COMMIT=`echo $TORCHVISION_VERSION | cut -d'+' -f2` && \
    TORCHVISION_PATH="/usr/local/lib/python3.6/dist-packages/torchvision-$TORCHVISION_VERSION-py3.6-linux-aarch64.egg" && \
    PYTORCH_PATH="/usr/local/lib/python3.6/dist-packages/torch" && \
    echo "TORCHVISION_VERSION = $TORCHVISION_VERSION" && \
    echo "TORCHVISION_COMMIT  = $TORCHVISION_COMMIT" && \
    echo "TORCHVISION_PATH    = $TORCHVISION_PATH" && \
    echo "PYTORCH_PATH        = $PYTORCH_PATH" && \
    git clone https://github.com/pytorch/vision torchvision && \
    cd torchvision && \
    git checkout $TORCHVISION_COMMIT && \
    cd ../ && \
    ln -s /tmp/torchvision/torchvision/csrc /tmp/torchvision/torchvision/torchvision && \
    ln -s $TORCHVISION_PATH/torchvision/_C.so $TORCHVISION_PATH/torchvision/libtorchvision.so && \
    git clone --branch ${TRITON_PYTORCH_BACKEND_VERSION} https://github.com/triton-inference-server/pytorch_backend && \
    cp /tmp/tritonserver/pytorch_backend/CMakeLists.txt /tmp/pytorch_backend/CMakeLists.txt && \
    cd pytorch_backend && \
    mkdir build && \
    cd build && \
    PYTORCH_PATH="/usr/local/lib/python3.6/dist-packages/torch" && \
    echo $PYTORCH_PATH && \
    cmake \
	  -DCMAKE_INSTALL_PREFIX:PATH=`pwd`/install \
	  -DTRITON_PYTORCH_INCLUDE_PATHS="$PYTORCH_PATH/include;$PYTORCH_PATH/include/torch/csrc/api/include;/tmp/torchvision/torchvision" \
	  -DTRITON_PYTORCH_LIB_PATHS="$PYTORCH_PATH/lib;$TORCHVISION_PATH/torchvision" \ 
	  .. && \
    make -j$(nproc) install && \
    cp -r install/backends/pytorch /opt/tritonserver/backends && \
    cd ../../ && \
    rm -r -f pytorch_backend && \
    rm -r -f torchvision 
    
    
# disable JupyterLab from auto-starting (inherited behavior from l4t-ml)
CMD /bin/bash
    